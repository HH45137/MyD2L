# 动手学深度学习

---

## 线性回归

- 回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。
- 回归经常用来表示输入和输出之间的关系。
- 机器学习中的大部分任务都与预测（prediction）相关，但是不是所有的预测都是回归问题。

## 线性回归的基本元素

- 线性回归最简单和流行
- 线性回归基于几个简单的假设：
    1. 假设自变量 ${x}$ 和因变量 ${y}$ 之间的关系是线性的，即 ${y}$ 可以表示为 ${x}$ 中元素的加权和，这里通常允许包含观察值的一些噪声。
    2. 假设任何噪声都比较正常，如遵循正态分布。
- 举一个实际的例子，我们希望根据房屋的面积和房龄来估算房屋价格。
    1. 要开发这个模型，我们需要收集一个真实的数据集。包括房屋的销售价格，面积，和房龄。在机器学习的术语中，该数据集称为训练集（training
       data set）或训练集（training set）。
    2. 每行数据（比如一次房屋交易相对应的数据）称为样本（sample），也称为数据点（data point）或数据样本（data instance）。
    3. 我们把试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。
    4. 预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）.

## 线性模型

1. 线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和，如下：

   $$
   {price = w_{area} \cdot area + w_{age} \cdot age + b}
   $$

   其中 ${w_{area}}$ 和 ${w_{age}}$ 称为权重，权重决定了每个特征对我们预测值的影响，${b}$
   称为偏置（bias），偏移量（offset）和截距（intercept）。偏置是指当所有特征都取值为0时，预测值该是多少，偏置无论如何不能少。如果没有偏置项，我们模型的表达能力将受到限制。严格来说，该公式是仿射变换，即线性变换（加权和）加上平移（偏置）。
2. 给定一个数据集，目标是寻找模型的权重和偏置，使得模型的输出也就是预测大致符合真实的数据。
3. 在机器学习领域，通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。
4. 将所有的特征放到向量 ${X \in \mathbb{R}^d}$ 中，并将所有权重放到向量 ${W \in \mathbb{R}^d}$ 中，可以用点积形式来简单地表达模型。

   $$
   {\hat{y} = w^\top x + b}
   $$

   其中向量 ${x}$ 对应单个数据样本的特征。用符号表示的矩阵 ${X \in \mathbb{R}^{n \times d}}$ 可以很方便的引用我们整个数据集的
   ${n}$ 个样本。其中 ${X}$ 的每一行是一个样本，每一列是一种特征。
5. 对于特征集合 ${X}$，预测值 ${\hat{y} \in \mathbb{R}^n }$ 可以通过矩阵-向量乘法表示为：

   $$
   {\hat{y}= Xw+b}
   $$
6. 因为无论如何观测特征 ${X}$ 和标签 ${y}$，都可能会出现少量的观测误差。因此，即使特征和标签的关系是线性的，我们也需要加入一个噪声项来考虑误差带来的影响。
7. 在开始寻找最优质的模型参数（权重和偏置）之前，我们还需要

    - 一种模型质量的度量方式
    - 一种能够更新模型以提高模型预测质量的方法

## 损失函数

1. 损失函数（loss function）能量化目标的实际值与预测值之间的差距
2. 回归问题中最常用的损失函数是平方误差函数。
3. 当样本 ${i}$ 的预测值为 ${\hat{y}^{(i)}}$ 其相应的真实标签为 ${y^{(i)}}$ 时，平方误差可以定义为以下公式：

   $$
   {l^{(i)} (w,b)=\frac{1}{2} \bigg(\hat{y}^{(i)} - y^{(i)}\bigg)^2}
   $$
4. 为了度量模型在整个数据集而不是单独的一个数据项上的质量，我们需要计算在训练集 ${n}$ 个样本上的损失均值（也等价于求和）
   ![fitlinreg.jpg](assets/fit-linreg.jpg)

   $$
   {L(w,b) = \frac{1}{n} \sum _{i=1}^n l^{(i)}(w,b) = \frac{1}{n} \sum _{i=1}^n \frac{1}{2} \bigg(w^\top x^{(i)} + b -
   y^{(i)} \bigg)^2}
   $$
5. 训练模型时，我们希望寻找一组参数 ${(w^*,b^*)}$ 这组参数能最小化在所有训练样本上的总损失。如下：

   $$
   {w^*,b^* = \argmin _{w,b} L(w,b)}
   $$

## 解析解

1. 线性回归的解可以用一个公式简单的表达出来，这类解叫做解析解（analytical solution）
2. 线性回归这样的简单问题存在解析解，但是并不是所有的问题都有解析解，因为其对问题的限制严格，导致它无法使用在深度学习中

## 随机梯度下降

1. 即使无法获得解析解，我们依然可以有效的训练模型。在这里，我们使用一种名为梯度下降（gradient
   descent）的方法，它通过不断地在损失函数递减的方向上跟新参数来降低误差。
